{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da3f885",
   "metadata": {},
   "source": [
    "## üîç Step 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fdbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU NOT available. Training will be VERY slow (2-3 hours).\")\n",
    "    print(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281832b",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets scikit-learn pandas numpy torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a693ba6",
   "metadata": {},
   "source": [
    "## üìö Step 3: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    CamembertTokenizer, \n",
    "    CamembertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549962d7",
   "metadata": {},
   "source": [
    "## üìÇ Step 4: Upload and Load Training Data\n",
    "\n",
    "**IMPORTANT**: Upload your `training_data.csv` file using the Files tab (üìÅ on the left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64991a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training_data.csv exists\n",
    "if not os.path.exists('training_data.csv'):\n",
    "    print(\"‚ö†Ô∏è ERROR: training_data.csv not found!\")\n",
    "    print(\"\"\"\\nüì§ Please upload your training_data.csv file:\n",
    "1\n",
    ",\n",
    "2\n",
    ",\n",
    "3\n",
    ",\n",
    "4\n",
    ",\n",
    "5\n",
    "\"\")\n",
    "    raise FileNotFoundError(\"training_data.csv not found\")\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv('training_data.csv')\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nüìà Class Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nüìù Sample data:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Validate data\n",
    "valid_labels = ['blood_test', 'xray', 'mri', 'prescription', \n",
    "                'medical_report', 'lab_result', 'consultation_note']\n",
    "invalid = df[~df['label'].isin(valid_labels)]\n",
    "if len(invalid) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {len(invalid)} invalid labels found!\")\n",
    "    print(invalid[['text', 'label']].head())\n",
    "else:\n",
    "    print(\"\\n‚úÖ All labels are valid!\")\n",
    "\n",
    "# Check for minimum samples per class\n",
    "counts = df['label'].value_counts()\n",
    "min_samples = counts.min()\n",
    "if min_samples < 20:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Some classes have very few samples (min: {min_samples})\")\n",
    "    print(\"   Recommendation: Add more samples for better accuracy\")\n",
    "elif min_samples < 50:\n",
    "    print(f\"\\n‚ö†Ô∏è Note: Min samples per class: {min_samples}\")\n",
    "    print(\"   Recommendation: 50-100 samples per class for best results\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Good! Min samples per class: {min_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e1520",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 5: Prepare Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define document types and label mapping\n",
    "DOCUMENT_TYPES = [\n",
    "    'blood_test',        # Analyse de sang\n",
    "    'xray',              # Radiographie\n",
    "    'mri',               # IRM\n",
    "    'prescription',      # Ordonnance\n",
    "    'medical_report',    # Rapport m√©dical\n",
    "    'lab_result',        # R√©sultat de laboratoire\n",
    "    'consultation_note'  # Note de consultation\n",
    "]\n",
    "\n",
    "# Create label to ID mapping\n",
    "label2id = {label: idx for idx, label in enumerate(DOCUMENT_TYPES)}\n",
    "id2label = {idx: label for idx, label in enumerate(DOCUMENT_TYPES)}\n",
    "\n",
    "# Convert labels to numeric IDs\n",
    "df['label_id'] = df['label'].map(label2id)\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df['text'].tolist()\n",
    "labels = df['label_id'].tolist()\n",
    "\n",
    "print(f\"‚úÖ Data prepared!\")\n",
    "print(f\"   Number of classes: {len(DOCUMENT_TYPES)}\")\n",
    "print(f\"   Label mapping: {label2id}\")\n",
    "print(f\"\\nüìù Example:\")\n",
    "print(f\"   Text: {texts[0][:100]}...\")\n",
    "print(f\"   Label: {df['label'].iloc[0]} ‚Üí ID: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd8f4a",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 6: Split Data (Train/Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels  # Ensure balanced split\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data split completed!\")\n",
    "print(f\"   Training samples: {len(train_texts)}\")\n",
    "print(f\"   Validation samples: {len(val_texts)}\")\n",
    "print(f\"\\nüìä Training set distribution:\")\n",
    "train_df = pd.DataFrame({'label_id': train_labels})\n",
    "print(train_df['label_id'].value_counts().sort_index())\n",
    "print(f\"\\nüìä Validation set distribution:\")\n",
    "val_df = pd.DataFrame({'label_id': val_labels})\n",
    "print(val_df['label_id'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe8fb67",
   "metadata": {},
   "source": [
    "## üî§ Step 7: Load Tokenizer and Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3250239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CamemBERT tokenizer (French BERT)\n",
    "MODEL_NAME = \"camembert-base\"\n",
    "tokenizer = CamembertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded: {MODEL_NAME}\")\n",
    "\n",
    "# Define custom dataset class\n",
    "class DocumentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DocumentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = DocumentDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets created!\")\n",
    "print(f\"   Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"   Val dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Test tokenization\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nüìù Sample tokenized data:\")\n",
    "print(f\"   Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"   Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"   Label: {sample['labels'].item()} ‚Üí {id2label[sample['labels'].item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba193c7",
   "metadata": {},
   "source": [
    "## üéØ Step 8: Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e788375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 8  # Reduce to 4 if out of memory\n",
    "NUM_EPOCHS = 10  # More epochs = better accuracy (but longer training)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders created!\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Total epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940e2a6",
   "metadata": {},
   "source": [
    "## ü§ñ Step 9: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CamemBERT model\n",
    "model = CamembertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(DOCUMENT_TYPES),\n",
    "    hidden_dropout_prob=0.3,  # Dropout for regularization\n",
    "    attention_probs_dropout_prob=0.3\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model initialized!\")\n",
    "print(f\"   Base model: {MODEL_NAME}\")\n",
    "print(f\"   Number of classes: {len(DOCUMENT_TYPES)}\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8086d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 10: Setup Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a936be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Optimizer and scheduler configured!\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"   Total training steps: {total_steps}\")\n",
    "print(f\"   Weight decay: 0.01 (L2 regularization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99877b8",
   "metadata": {},
   "source": [
    "## üöÄ Step 11: Training Loop\n",
    "\n",
    "**This will take 10-15 minutes with GPU** (or 2-3 hours without GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694bc52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
    "    \n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# Validation function\n",
    "def eval_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
    "    \n",
    "    return avg_loss, accuracy.item(), all_preds, all_labels\n",
    "\n",
    "# Main training loop\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "best_val_acc = 0\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    print(f\"\\nüìä Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = eval_epoch(model, val_loader, device)\n",
    "    print(f\"üìä Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        print(f\"\\nüèÜ New best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Save history\n",
    "    training_history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc\n",
    "    })\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"‚úÖ Training Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"üèÜ Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed13fb",
   "metadata": {},
   "source": [
    "## üìä Step 12: Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced83639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final predictions\n",
    "_, final_acc, final_preds, final_labels = eval_epoch(model, val_loader, device)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "report = classification_report(\n",
    "    final_labels, \n",
    "    final_preds, \n",
    "    target_names=DOCUMENT_TYPES,\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nüî¢ Confusion Matrix:\")\n",
    "print(\"=\"*60)\n",
    "cm = confusion_matrix(final_labels, final_preds)\n",
    "print(\"\\nRows = True labels, Columns = Predicted labels\")\n",
    "print(f\"\\n{' '*20}\", end=\"\")\n",
    "for label in DOCUMENT_TYPES:\n",
    "    print(f\"{label[:8]:>10}\", end=\"\")\n",
    "print()\n",
    "for i, label in enumerate(DOCUMENT_TYPES):\n",
    "    print(f\"{label:>20}\", end=\"\")\n",
    "    for j in range(len(DOCUMENT_TYPES)):\n",
    "        print(f\"{cm[i][j]:>10}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\n\\nüìà Per-Class Accuracy:\")\n",
    "print(\"=\"*60)\n",
    "for i, label in enumerate(DOCUMENT_TYPES):\n",
    "    class_mask = np.array(final_labels) == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = (np.array(final_preds)[class_mask] == i).sum() / class_mask.sum()\n",
    "        print(f\"{label:>20}: {class_acc:.4f} ({class_acc*100:.2f}%)\")\n",
    "\n",
    "# Training history\n",
    "print(\"\\n\\nüìâ Training History:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Epoch':<10} {'Train Loss':<15} {'Train Acc':<15} {'Val Loss':<15} {'Val Acc':<15}\")\n",
    "print(\"-\"*70)\n",
    "for h in training_history:\n",
    "    print(f\"{h['epoch']:<10} {h['train_loss']:<15.4f} {h['train_acc']:<15.4f} {h['val_loss']:<15.4f} {h['val_acc']:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f70b88",
   "metadata": {},
   "source": [
    "## üíæ Step 13: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory\n",
    "OUTPUT_DIR = \"document_classifier_model\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save model state dict (PyTorch format)\n",
    "model_path = os.path.join(OUTPUT_DIR, \"model.pth\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'label_map': label2id,\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'training_history': training_history\n",
    "}, model_path)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Save config with metadata\n",
    "config_data = {\n",
    "    'base_model': MODEL_NAME,\n",
    "    'num_labels': len(DOCUMENT_TYPES),\n",
    "    'document_types': DOCUMENT_TYPES,\n",
    "    'label_map': label2id,\n",
    "    'id_to_label': id2label,\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'training_samples': len(train_texts),\n",
    "    'validation_samples': len(val_texts),\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'max_length': 512\n",
    "}\n",
    "\n",
    "config_path = os.path.join(OUTPUT_DIR, \"config.json\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {OUTPUT_DIR}/\")\n",
    "print(f\"   Files created:\")\n",
    "print(f\"   - model.pth (model weights)\")\n",
    "print(f\"   - config.json (metadata)\")\n",
    "print(f\"   - tokenizer files\")\n",
    "\n",
    "# Create zip file for easy download\n",
    "zip_filename = \"document_classifier_model.zip\"\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, os.path.dirname(OUTPUT_DIR))\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"\\nüì¶ Zip file created: {zip_filename}\")\n",
    "print(f\"   Size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB\")\n",
    "print(f\"\\nüì• Download instructions:\")\n",
    "print(f\"   1. Click on Files tab (üìÅ)\")\n",
    "print(f\"   2. Find {zip_filename}\")\n",
    "print(f\"   3. Right-click ‚Üí Download\")\n",
    "print(f\"   4. Extract on your computer\")\n",
    "print(f\"   5. Move to: backend/ml_service/saved_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f113982",
   "metadata": {},
   "source": [
    "## üß™ Step 14: Test Model with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_document(text, model, tokenizer, device, label2id, id2label):\n",
    "    \"\"\"Predict document type for a given text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        pred_id = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0][pred_id].item()\n",
    "    \n",
    "    return id2label[pred_id], confidence, probs[0].cpu().numpy()\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"R√©sultats de l'analyse sanguine: H√©moglobine 14.5 g/dL, Leucocytes 7200/mm¬≥, Plaquettes 250000/mm¬≥\",\n",
    "    \"Radiographie thoracique de face: Poumons clairs sans opacit√© parenchymateuse. C≈ìur de taille normale.\",\n",
    "    \"IRM c√©r√©brale avec injection de gadolinium: Pas de processus expansif intracr√¢nien. Examen normal.\",\n",
    "    \"ORDONNANCE: AMOXICILLINE 1g, 1 comprim√© 3 fois par jour pendant 7 jours. PARACETAMOL 1g si douleur.\",\n",
    "    \"Compte-rendu d'hospitalisation: Patient admis le 12/03/2024 pour dyspn√©e aigu√´ et douleur thoracique.\",\n",
    "    \"R√©sultats laboratoire: HbA1c 7.2%, Cholest√©rol total 2.10 g/L, TSH 2.8 mUI/L, Cr√©atinin√©mie 92 ¬µmol/L.\",\n",
    "    \"Note de consultation: Patient √¢g√© de 45 ans consultant pour lombalgies chroniques √©voluant depuis 6 mois.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model with examples:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, text in enumerate(test_examples, 1):\n",
    "    pred_label, confidence, all_probs = predict_document(\n",
    "        text, model, tokenizer, device, label2id, id2label\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Text: {text[:80]}...\")\n",
    "    print(f\"Predicted: {pred_label} (confidence: {confidence:.4f})\")\n",
    "    \n",
    "    # Show top 3 predictions\n",
    "    top3_indices = np.argsort(all_probs)[-3:][::-1]\n",
    "    print(f\"Top 3 predictions:\")\n",
    "    for idx in top3_indices:\n",
    "        print(f\"  {id2label[idx]:>20}: {all_probs[idx]:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebe435",
   "metadata": {},
   "source": [
    "## üéâ Summary and Next Steps\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ‚úÖ Loaded and validated your training data\n",
    "- ‚úÖ Split data into train/validation sets\n",
    "- ‚úÖ Trained a CamemBERT model on French medical documents\n",
    "- ‚úÖ Achieved validation accuracy (check above)\n",
    "- ‚úÖ Saved model as downloadable ZIP file\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download model**: Files tab ‚Üí document_classifier_model.zip ‚Üí Download\n",
    "2. **Extract on your computer**\n",
    "3. **Move to project**: `C:\\docqa-ms\\backend\\ml_service\\saved_models\\`\n",
    "4. **Restart ML service**: `docker-compose restart ml-service`\n",
    "5. **Test API**: Use Postman or curl to test classification endpoint\n",
    "\n",
    "### If Accuracy is Low (<80%):\n",
    "- **Add more training data** (50-100 samples per class)\n",
    "- **Increase NUM_EPOCHS** to 15-20\n",
    "- **Check data quality** (correct labels, clean text)\n",
    "- **Balance classes** (equal samples per class)\n",
    "- **Try data augmentation** (back-translation, synonym replacement)\n",
    "\n",
    "### Commands for Deployment:\n",
    "```powershell\n",
    "# PowerShell (Windows)\n",
    "cd C:\\docqa-ms\n",
    "Expand-Archive -Path \"$env:USERPROFILE\\Downloads\\document_classifier_model.zip\" -DestinationPath \".\\backend\\ml_service\\saved_models\\\"\n",
    "docker-compose restart ml-service\n",
    "docker logs ml-service --tail 30\n",
    "```\n",
    "\n",
    "**Good luck with your project!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383f5c6",
   "metadata": {},
   "source": [
    "## üîç Step 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e957a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU NOT available. Training will be VERY slow (2-3 hours).\")\n",
    "    print(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948de37",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets scikit-learn pandas numpy torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a212d5b",
   "metadata": {},
   "source": [
    "## üìö Step 3: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2344a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132b8c1",
   "metadata": {},
   "source": [
    "## üìÇ Step 4: Upload and Load Training Data\n",
    "\n",
    "**IMPORTANT**: Upload your `training_data.csv` file using the Files tab (üìÅ on the left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713638f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('training_data.csv'):\n",
    "    print(\"‚ö†Ô∏è ERROR: training_data.csv not found!\")\n",
    "    print(\"\\nüì§ Please upload your training_data.csv file:\")\n",
    "    print(\"    1. Click on Files tab (üìÅ on the left)\")\n",
    "    print(\"    2. Click Upload button\")\n",
    "    print(\"    3. Select your training_data.csv\")\n",
    "    print(\"    4. Wait for upload to complete\")\n",
    "    print(\"    5. Re-run this cell\")\n",
    "    raise FileNotFoundError(\"training_data.csv not found\")\n",
    "\n",
    "df = pd.read_csv('training_data.csv')\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nüìà Class Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nüìù Sample data:\")\n",
    "print(df.head(3))\n",
    "\n",
    "valid_labels = ['blood_test', 'xray', 'mri', 'prescription', 'medical_report', 'lab_result', 'consultation_note']\n",
    "invalid = df[~df['label'].isin(valid_labels)]\n",
    "if len(invalid) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {len(invalid)} invalid labels found!\")\n",
    "    print(invalid[['text', 'label']].head())\n",
    "else:\n",
    "    print(\"\\n‚úÖ All labels are valid!\")\n",
    "\n",
    "counts = df['label'].value_counts()\n",
    "min_samples = counts.min()\n",
    "if min_samples < 20:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Some classes have very few samples (min: {min_samples})\")\n",
    "    print(\"   Recommendation: Add more samples for better accuracy\")\n",
    "elif min_samples < 50:\n",
    "    print(f\"\\n‚ö†Ô∏è Note: Min samples per class: {min_samples}\")\n",
    "    print(\"   Recommendation: 50-100 samples per class for best results\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Good! Min samples per class: {min_samples}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
