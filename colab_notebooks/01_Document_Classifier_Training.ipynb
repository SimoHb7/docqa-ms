{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè• Document Classifier Training - French Medical Documents\n",
        "\n",
        "Train **CamemBERT** to classify 7 types of medical documents\n",
        "\n",
        "## ‚ö° Quick Start\n",
        "1. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
        "2. Upload training_data.csv (Files tab)\n",
        "3. Run all cells\n",
        "4. Download model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU NOT available. Enable: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers datasets scikit-learn pandas numpy torch -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ All libraries imported!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "if not os.path.exists('training_data.csv'):\n",
        "    raise FileNotFoundError(\"Upload training_data.csv using Files tab\")\n",
        "\n",
        "df = pd.read_csv('training_data.csv')\n",
        "print(f\"‚úÖ Loaded {len(df)} samples\")\n",
        "print(f\"\\nClass distribution:\\n{df['label'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define document types\n",
        "DOCUMENT_TYPES = ['blood_test', 'xray', 'mri', 'prescription', 'medical_report', 'lab_result', 'consultation_note']\n",
        "\n",
        "# Create label mappings\n",
        "label2id = {label: idx for idx, label in enumerate(DOCUMENT_TYPES)}\n",
        "id2label = {idx: label for idx, label in enumerate(DOCUMENT_TYPES)}\n",
        "\n",
        "# Convert labels to IDs\n",
        "df['label_id'] = df['label'].map(label2id)\n",
        "texts = df['text'].tolist()\n",
        "labels = df['label_id'].tolist()\n",
        "\n",
        "print(f\"‚úÖ Labels: {label2id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data (80% train, 20% validation)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train: {len(train_texts)}, Val: {len(val_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "MODEL_NAME = \"camembert-base\"\n",
        "tokenizer = CamembertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Create dataset class\n",
        "class DocumentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            str(self.texts[idx]),\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = DocumentDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = DocumentDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "print(f\"‚úÖ Datasets created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"‚úÖ DataLoaders ready: {len(train_loader)} train batches, {len(val_loader)} val batches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = CamembertForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(DOCUMENT_TYPES),\n",
        "    hidden_dropout_prob=0.3,\n",
        "    attention_probs_dropout_prob=0.3\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"‚úÖ Model initialized on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup optimizer and scheduler\n",
        "LEARNING_RATE = 2e-5\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=100, num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Optimizer ready (LR: {LEARNING_RATE})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and validation functions\n",
        "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        \n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += torch.sum(preds == labels)\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(data_loader), correct.double() / len(data_loader.dataset)\n",
        "\n",
        "def eval_epoch(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            \n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += torch.sum(preds == labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    return total_loss / len(data_loader), correct.double() / len(data_loader.dataset), all_preds, all_labels\n",
        "\n",
        "# Main training loop\n",
        "print(\"üöÄ Starting training...\\n\")\n",
        "best_val_acc = 0\n",
        "training_history = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    \n",
        "    val_loss, val_acc, val_preds, val_labels = eval_epoch(model, val_loader, device)\n",
        "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "    \n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        print(f\"üèÜ New best: {best_val_acc:.4f}\")\n",
        "    \n",
        "    training_history.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc.item(),\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc.item()\n",
        "    })\n",
        "\n",
        "print(f\"\\n‚úÖ Training complete! Best accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(\"=\"*60)\n",
        "report = classification_report(val_labels, val_preds, target_names=DOCUMENT_TYPES, digits=4)\n",
        "print(report)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nüî¢ Confusion Matrix:\")\n",
        "cm = confusion_matrix(val_labels, val_preds)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "OUTPUT_DIR = \"document_classifier_model\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Save PyTorch model with wrapped state dict\n",
        "model_path = os.path.join(OUTPUT_DIR, \"model.pth\")\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'label_map': label2id,\n",
        "    'best_val_acc': best_val_acc.item(),\n",
        "    'training_history': training_history\n",
        "}, model_path)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Save config\n",
        "config_data = {\n",
        "    'base_model': MODEL_NAME,\n",
        "    'num_labels': len(DOCUMENT_TYPES),\n",
        "    'document_types': DOCUMENT_TYPES,\n",
        "    'label_map': label2id,\n",
        "    'id_to_label': id2label,\n",
        "    'best_val_acc': best_val_acc.item(),\n",
        "    'training_samples': len(train_texts),\n",
        "    'validation_samples': len(val_texts),\n",
        "    'num_epochs': NUM_EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'max_length': 512\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"config.json\"), 'w', encoding='utf-8') as f:\n",
        "    json.dump(config_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Create zip file\n",
        "zip_filename = \"document_classifier_model.zip\"\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(OUTPUT_DIR):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, os.path.dirname(OUTPUT_DIR))\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"‚úÖ Model saved! Download: {zip_filename}\")\n",
        "print(f\"Size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with examples\n",
        "def predict(text):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = torch.softmax(outputs.logits, dim=1)\n",
        "        pred_id = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs[0][pred_id].item()\n",
        "    \n",
        "    return id2label[pred_id], confidence\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"R√©sultats analyse sanguine: H√©moglobine 14.5 g/dL, Leucocytes 7200/mm¬≥\",\n",
        "    \"Radiographie thoracique: Poumons clairs sans opacit√©\",\n",
        "    \"IRM c√©r√©brale: Pas de processus expansif intracr√¢nien\",\n",
        "    \"ORDONNANCE: AMOXICILLINE 1g, 3 fois par jour\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing predictions:\\n\")\n",
        "for text in test_texts:\n",
        "    label, conf = predict(text)\n",
        "    print(f\"Text: {text[:60]}...\")\n",
        "    print(f\"Predicted: {label} (confidence: {conf:.4f})\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}