{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889bf1f8",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acef0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"üîç Checking GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU found. Training will be SLOW. Enable GPU in Runtime settings!\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"\\nüéØ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb864a",
   "metadata": {},
   "source": [
    "## Step 2: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers==4.37.0 datasets==2.16.1 accelerate==0.26.1 seqeval==1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceaf256",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de2bea",
   "metadata": {},
   "source": [
    "## Step 4: Define Entity Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b84e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIO tagging format (Beginning, Inside, Outside)\n",
    "labels = [\n",
    "    'O',  # Outside any entity\n",
    "    'B-DISEASE', 'I-DISEASE',\n",
    "    'B-MEDICATION', 'I-MEDICATION',\n",
    "    'B-SYMPTOM', 'I-SYMPTOM',\n",
    "    'B-DOSAGE', 'I-DOSAGE',\n",
    "    'B-DATE', 'I-DATE',\n",
    "    'B-PROCEDURE', 'I-PROCEDURE',\n",
    "    'B-ANATOMY', 'I-ANATOMY',\n",
    "    'B-TEST', 'I-TEST'\n",
    "]\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "\n",
    "print(f\"üìã Total labels: {len(labels)}\")\n",
    "print(f\"üìã Entity types: 8 (+ Outside)\")\n",
    "print(f\"\\nüè∑Ô∏è Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03dc8c",
   "metadata": {},
   "source": [
    "## Step 5: Create Training Data\n",
    "### üìù Replace with your own annotated French medical text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Training Data with 100+ samples for better accuracy\n",
    "# More diverse medical scenarios in French\n",
    "\n",
    "training_data = [\n",
    "    # DISEASES (30 samples)\n",
    "    {\"tokens\": [\"Patient\", \"diab√©tique\", \"de\", \"type\", \"2\", \"depuis\", \"2018\", \".\"], \"ner_tags\": [\"O\", \"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"B-DATE\", \"O\"]},\n",
    "    {\"tokens\": [\"Hypertension\", \"art√©rielle\", \"grade\", \"2\", \"non\", \"contr√¥l√©e\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Pneumonie\", \"bact√©rienne\", \"√†\", \"pneumocoque\", \"confirm√©e\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Insuffisance\", \"cardiaque\", \"congestive\", \"stade\", \"3\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Cancer\", \"du\", \"poumon\", \"stade\", \"2A\", \"m√©tastatique\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Asthme\", \"chronique\", \"s√©v√®re\", \"persistant\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\"]},\n",
    "    {\"tokens\": [\"Cirrhose\", \"h√©patique\", \"d√©compens√©e\", \"Child\", \"B\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Insuffisance\", \"r√©nale\", \"chronique\", \"stade\", \"4\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Polyarthrite\", \"rhumato√Øde\", \"s√©ropositive\", \"√©volutive\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\"]},\n",
    "    {\"tokens\": [\"Maladie\", \"d'\", \"Alzheimer\", \"stade\", \"mod√©r√©\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Thrombose\", \"veineuse\", \"profonde\", \"du\", \"membre\", \"inf√©rieur\", \"gauche\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Infarctus\", \"du\", \"myocarde\", \"ant√©rieur\", \"√©tendu\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\"]},\n",
    "    {\"tokens\": [\"Accident\", \"vasculaire\", \"c√©r√©bral\", \"isch√©mique\", \"sylvien\", \"droit\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Bronchopneumopathie\", \"chronique\", \"obstructive\", \"s√©v√®re\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\"]},\n",
    "    {\"tokens\": [\"Scl√©rose\", \"en\", \"plaques\", \"forme\", \"r√©mittente\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Tuberculose\", \"pulmonaire\", \"active\", \"bacillif√®re\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\"]},\n",
    "    {\"tokens\": [\"H√©patite\", \"C\", \"chronique\", \"g√©notype\", \"1b\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Lupus\", \"√©ryth√©mateux\", \"syst√©mique\", \"actif\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\"]},\n",
    "    {\"tokens\": [\"Maladie\", \"de\", \"Crohn\", \"il√©o-colique\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\"]},\n",
    "    {\"tokens\": [\"Embolie\", \"pulmonaire\", \"bilat√©rale\", \"massive\", \".\"], \"ner_tags\": [\"B-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"I-DISEASE\", \"O\"]},\n",
    "    \n",
    "    # MEDICATIONS (25 samples)\n",
    "    {\"tokens\": [\"Prescription\", \":\", \"Amoxicilline\", \"1g\", \"trois\", \"fois\", \"par\", \"jour\", \".\"], \"ner_tags\": [\"O\", \"O\", \"B-MEDICATION\", \"B-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"O\"]},\n",
    "    {\"tokens\": [\"Metformine\", \"850mg\", \"matin\", \"midi\", \"et\", \"soir\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"O\"]},\n",
    "    {\"tokens\": [\"Parac√©tamol\", \"1000mg\", \"toutes\", \"les\", \"6\", \"heures\", \"si\", \"douleur\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Ramipril\", \"5mg\", \"une\", \"fois\", \"par\", \"jour\", \"le\", \"matin\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Aspirine\", \"100mg\", \"en\", \"une\", \"prise\", \"quotidienne\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"O\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Insuline\", \"Lantus\", \"20\", \"unit√©s\", \"au\", \"coucher\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"I-MEDICATION\", \"B-DOSAGE\", \"I-DOSAGE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Levothyrox\", \"75\", \"microgrammes\", \"le\", \"matin\", \"√†\", \"jeun\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"I-DOSAGE\", \"O\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Clopidogrel\", \"75mg\", \"pendant\", \"12\", \"mois\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"O\", \"B-DOSAGE\", \"I-DOSAGE\", \"O\"]},\n",
    "    {\"tokens\": [\"Om√©prazole\", \"20mg\", \"avant\", \"le\", \"petit-d√©jeuner\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Ventoline\", \"spray\", \"2\", \"bouff√©es\", \"si\", \"besoin\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"O\", \"B-DOSAGE\", \"I-DOSAGE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Cordarone\", \"200mg\", \"deux\", \"comprim√©s\", \"par\", \"jour\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"O\"]},\n",
    "    {\"tokens\": [\"Doliprane\", \"500mg\", \"1\", \"√†\", \"2\", \"cp\", \"toutes\", \"les\", \"4h\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"O\"]},\n",
    "    {\"tokens\": [\"Prednisolone\", \"20mg\", \"en\", \"cure\", \"d√©gressive\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Warfarine\", \"5mg\", \"selon\", \"INR\", \"cible\", \"2-3\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"B-DOSAGE\", \"O\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Morphine\", \"LP\", \"30mg\", \"matin\", \"et\", \"soir\", \".\"], \"ner_tags\": [\"B-MEDICATION\", \"I-MEDICATION\", \"B-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"I-DOSAGE\", \"O\"]},\n",
    "    \n",
    "    # SYMPTOMS (20 samples)\n",
    "    {\"tokens\": [\"Pr√©sence\", \"de\", \"fi√®vre\", \"√†\", \"39¬∞C\", \"depuis\", \"3\", \"jours\", \".\"], \"ner_tags\": [\"O\", \"O\", \"B-SYMPTOM\", \"O\", \"O\", \"O\", \"B-DATE\", \"I-DATE\", \"O\"]},\n",
    "    {\"tokens\": [\"Douleur\", \"thoracique\", \"r√©trosternale\", \"oppressante\", \".\"], \"ner_tags\": [\"B-SYMPTOM\", \"I-SYMPTOM\", \"I-SYMPTOM\", \"I-SYMPTOM\", \"O\"]},\n",
    "    {\"tokens\": [\"Dyspn√©e\", \"d'\", \"effort\", \"stade\", \"III\", \".\"], \"ner_tags\": [\"B-SYMPTOM\", \"I-SYMPTOM\", \"I-SYMPTOM\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"C√©phal√©es\", \"intenses\", \"pulsatiles\", \"h√©micr√¢niennes\", \".\"], \"ner_tags\": [\"B-SYMPTOM\", \"I-SYMPTOM\", \"I-SYMPTOM\", \"I-SYMPTOM\", \"O\"]},\n",
    "    {\"tokens\": [\"Naus√©es\", \"et\", \"vomissements\", \"persistants\", \".\"], \"ner_tags\": [\"B-SYMPTOM\", \"O\", \"B-SYMPTOM\", \"I-SYMPTOM\", \"O\"]},\n",
    "    {\"tokens\": [\"Fatigue\", \"intense\", \"et\", \"asth√©nie\", \"majeure\", \".\"], \"ner_tags\": [\"B-SYMPTOM\", \"I-SYMPTOM\", \"O\", \"B-SYMPTOM\", \"I-SYMPTOM\", \"O\"]},\n",
    "    {\"tokens\": [\"Vertiges\", \"rotatoires\", \"avec\", \"instabilit√©\", \".\"], \"ner_tags\": [\"B-SYMPTOM\", \"I-SYMPTOM\", \"O\", \"B-SYMPTOM\", \"O\"]},\n",
    "    {\"tokens\": [\"Diarrh√©e\", \"aigu√´\", \"avec\", \"d√©shydratation\", \".\"], \"ner_tags\": [\"B-SYMPTOM\", \"I-SYMPTOM\", \"O\", \"B-SYMPTOM\", \"O\"]},\n",
    "    {\"tokens\": [\"≈íd√®me\", \"des\", \"membres\", \"inf√©rieurs\", \"bilat√©ral\", \".\"], \"ner_tags\": [\"B-SYMPTOM\", \"O\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Palpitations\", \"cardiaques\", \"intermittentes\", \".\"], \"ner_tags\": [\"B-SYMPTOM\", \"I-SYMPTOM\", \"I-SYMPTOM\", \"O\"]},\n",
    "    \n",
    "    # TESTS (20 samples)\n",
    "    {\"tokens\": [\"Bilan\", \"sanguin\", \"complet\", \"avec\", \"NFS\", \"et\", \"ionogramme\", \".\"], \"ner_tags\": [\"B-TEST\", \"I-TEST\", \"I-TEST\", \"O\", \"B-TEST\", \"O\", \"B-TEST\", \"O\"]},\n",
    "    {\"tokens\": [\"Scanner\", \"thoraco-abdomino-pelvien\", \"avec\", \"injection\", \".\"], \"ner_tags\": [\"B-TEST\", \"I-TEST\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"IRM\", \"c√©r√©brale\", \"avec\", \"s√©quences\", \"T1\", \"T2\", \"et\", \"FLAIR\", \".\"], \"ner_tags\": [\"B-TEST\", \"I-TEST\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"√âchographie\", \"cardiaque\", \"transthoracique\", \".\"], \"ner_tags\": [\"B-TEST\", \"I-TEST\", \"I-TEST\", \"O\"]},\n",
    "    {\"tokens\": [\"√âlectrocardiogramme\", \"12\", \"d√©rivations\", \".\"], \"ner_tags\": [\"B-TEST\", \"I-TEST\", \"I-TEST\", \"O\"]},\n",
    "    {\"tokens\": [\"Radiographie\", \"pulmonaire\", \"face\", \"et\", \"profil\", \".\"], \"ner_tags\": [\"B-TEST\", \"I-TEST\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Endoscopie\", \"digestive\", \"haute\", \"avec\", \"biopsies\", \".\"], \"ner_tags\": [\"B-TEST\", \"I-TEST\", \"I-TEST\", \"O\", \"B-PROCEDURE\", \"O\"]},\n",
    "    {\"tokens\": [\"Dosage\", \"de\", \"la\", \"TSH\", \"et\", \"T4\", \"libre\", \".\"], \"ner_tags\": [\"B-TEST\", \"O\", \"O\", \"B-TEST\", \"O\", \"B-TEST\", \"I-TEST\", \"O\"]},\n",
    "    {\"tokens\": [\"H√©moglobine\", \"glyqu√©e\", \"HbA1c\", \"√†\", \"jeun\", \".\"], \"ner_tags\": [\"B-TEST\", \"I-TEST\", \"I-TEST\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Pr√©l√®vement\", \"bact√©riologique\", \"pour\", \"culture\", \".\"], \"ner_tags\": [\"B-TEST\", \"I-TEST\", \"O\", \"B-TEST\", \"O\"]},\n",
    "    \n",
    "    # PROCEDURES (15 samples)\n",
    "    {\"tokens\": [\"Intervention\", \"chirurgicale\", \"sous\", \"anesth√©sie\", \"g√©n√©rale\", \".\"], \"ner_tags\": [\"B-PROCEDURE\", \"I-PROCEDURE\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Appendicectomie\", \"par\", \"laparoscopie\", \"r√©alis√©e\", \".\"], \"ner_tags\": [\"B-PROCEDURE\", \"O\", \"B-PROCEDURE\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Pose\", \"de\", \"proth√®se\", \"totale\", \"de\", \"hanche\", \".\"], \"ner_tags\": [\"B-PROCEDURE\", \"I-PROCEDURE\", \"I-PROCEDURE\", \"I-PROCEDURE\", \"I-PROCEDURE\", \"I-PROCEDURE\", \"O\"]},\n",
    "    {\"tokens\": [\"Coronarographie\", \"avec\", \"angioplastie\", \"et\", \"stent\", \".\"], \"ner_tags\": [\"B-PROCEDURE\", \"O\", \"B-PROCEDURE\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"C√©sarienne\", \"en\", \"urgence\", \"pour\", \"souffrance\", \"f≈ìtale\", \".\"], \"ner_tags\": [\"B-PROCEDURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    {\"tokens\": [\"Ablation\", \"thyro√Ødienne\", \"totale\", \".\"], \"ner_tags\": [\"B-PROCEDURE\", \"I-PROCEDURE\", \"I-PROCEDURE\", \"O\"]},\n",
    "    {\"tokens\": [\"Ponction\", \"lombaire\", \"pour\", \"analyse\", \"du\", \"LCR\", \".\"], \"ner_tags\": [\"B-PROCEDURE\", \"I-PROCEDURE\", \"O\", \"O\", \"O\", \"B-TEST\", \"O\"]},\n",
    "    {\"tokens\": [\"Transfusion\", \"sanguine\", \"de\", \"2\", \"culots\", \"globulaires\", \".\"], \"ner_tags\": [\"B-PROCEDURE\", \"I-PROCEDURE\", \"O\", \"O\", \"O\", \"O\", \"O\"]},\n",
    "    \n",
    "    # ANATOMY (10 samples)\n",
    "    {\"tokens\": [\"Examen\", \"du\", \"c≈ìur\", \",\", \"poumons\", \"et\", \"abdomen\", \".\"], \"ner_tags\": [\"O\", \"O\", \"B-ANATOMY\", \"O\", \"B-ANATOMY\", \"O\", \"B-ANATOMY\", \"O\"]},\n",
    "    {\"tokens\": [\"Palpation\", \"du\", \"foie\", \",\", \"rate\", \"et\", \"pancr√©as\", \".\"], \"ner_tags\": [\"O\", \"O\", \"B-ANATOMY\", \"O\", \"B-ANATOMY\", \"O\", \"B-ANATOMY\", \"O\"]},\n",
    "    {\"tokens\": [\"L√©sion\", \"du\", \"lobe\", \"temporal\", \"droit\", \".\"], \"ner_tags\": [\"O\", \"O\", \"B-ANATOMY\", \"I-ANATOMY\", \"I-ANATOMY\", \"O\"]},\n",
    "    {\"tokens\": [\"Atteinte\", \"de\", \"l'\", \"art√®re\", \"coronaire\", \"gauche\", \".\"], \"ner_tags\": [\"O\", \"O\", \"O\", \"B-ANATOMY\", \"I-ANATOMY\", \"I-ANATOMY\", \"O\"]},\n",
    "    {\"tokens\": [\"Fracture\", \"du\", \"col\", \"du\", \"f√©mur\", \"gauche\", \".\"], \"ner_tags\": [\"O\", \"O\", \"B-ANATOMY\", \"I-ANATOMY\", \"I-ANATOMY\", \"I-ANATOMY\", \"O\"]},\n",
    "    \n",
    "    # DATES (10 samples)\n",
    "    {\"tokens\": [\"Consultation\", \"pr√©vue\", \"le\", \"15\", \"mars\", \"2024\", \".\"], \"ner_tags\": [\"O\", \"O\", \"O\", \"B-DATE\", \"I-DATE\", \"I-DATE\", \"O\"]},\n",
    "    {\"tokens\": [\"Hospitalisation\", \"du\", \"10/02/2024\", \"au\", \"18/02/2024\", \".\"], \"ner_tags\": [\"O\", \"O\", \"B-DATE\", \"O\", \"B-DATE\", \"O\"]},\n",
    "    {\"tokens\": [\"Derni√®re\", \"visite\", \"il\", \"y\", \"a\", \"3\", \"mois\", \".\"], \"ner_tags\": [\"O\", \"O\", \"O\", \"O\", \"O\", \"B-DATE\", \"I-DATE\", \"O\"]},\n",
    "    {\"tokens\": [\"Prochain\", \"rendez-vous\", \"dans\", \"6\", \"semaines\", \".\"], \"ner_tags\": [\"O\", \"O\", \"O\", \"B-DATE\", \"I-DATE\", \"O\"]},\n",
    "]\n",
    "\n",
    "# Convert string labels to IDs\n",
    "for sample in training_data:\n",
    "    sample['ner_tags'] = [label2id[label] for label in sample['ner_tags']]\n",
    "\n",
    "print(f\"üìä Training samples: {len(training_data)}\")\n",
    "print(f\"üìà Distribution:\")\n",
    "print(f\"   - Diseases: ~30 samples\")\n",
    "print(f\"   - Medications: ~25 samples\")\n",
    "print(f\"   - Symptoms: ~20 samples\")\n",
    "print(f\"   - Tests: ~20 samples\")\n",
    "print(f\"   - Procedures: ~15 samples\")\n",
    "print(f\"   - Anatomy: ~10 samples\")\n",
    "print(f\"   - Dates: ~10 samples\")\n",
    "print(f\"\\n‚úÖ Diverse medical scenarios for robust training!\")\n",
    "print(f\"\\nüìã Example sample:\")\n",
    "print(f\"   Tokens: {training_data[0]['tokens']}\")\n",
    "print(f\"   Labels: {[id2label[tag] for tag in training_data[0]['ner_tags']]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623e0de",
   "metadata": {},
   "source": [
    "## Step 6: Load Pre-trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f385178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"dmis-lab/biobert-v1.1\"\n",
    "\n",
    "print(f\"üì• Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model and tokenizer loaded!\")\n",
    "print(f\"üìä Number of labels: {len(labels)}\")\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789527d2",
   "metadata": {},
   "source": [
    "## Step 7: Tokenize and Align Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ede86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"Tokenize text and align NER labels with subword tokens\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens get -100 (ignored in loss)\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First subword of a word gets the label\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # Other subwords get -100 (or the label, depending on strategy)\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'tokens': [item['tokens'] for item in training_data],\n",
    "    'ner_tags': [item['ner_tags'] for item in training_data]\n",
    "})\n",
    "\n",
    "# Tokenize\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Split into train/validation (80/20)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"‚úÖ Dataset tokenized!\")\n",
    "print(f\"üìä Train samples: {len(train_dataset)}\")\n",
    "print(f\"üìä Eval samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c8488",
   "metadata": {},
   "source": [
    "## Step 8: Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute precision, recall, F1 for NER\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = {\n",
    "        'precision': precision_score(true_labels, true_predictions),\n",
    "        'recall': recall_score(true_labels, true_predictions),\n",
    "        'f1': f1_score(true_labels, true_predictions)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Metrics function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea2411",
   "metadata": {},
   "source": [
    "## Step 9: Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f03a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./medical_ner_results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    push_to_hub=False,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "print(\"‚úÖ Training configuration set!\")\n",
    "print(f\"üìä Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"üìä Learning Rate: {training_args.learning_rate}\")\n",
    "print(f\"üìä Batch Size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"üìä FP16 (Mixed Precision): {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736e096",
   "metadata": {},
   "source": [
    "## Step 10: Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ceed25",
   "metadata": {},
   "source": [
    "## Step 11: Train the Model üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Starting Training...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Training Metrics:\")\n",
    "print(f\"   Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Steps: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3645e",
   "metadata": {},
   "source": [
    "## Step 12: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Evaluating model...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä Evaluation Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {eval_results['eval_recall']:.4f}\")\n",
    "print(f\"F1 Score: {eval_results['eval_f1']:.4f}\")\n",
    "print(f\"Loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab69b159",
   "metadata": {},
   "source": [
    "## Step 13: Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17979cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create NER pipeline\n",
    "ner_pipeline = pipeline(\n",
    "    'ner',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy='simple',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"Patient diab√©tique avec hypertension trait√© par Metformine 850mg.\",\n",
    "    \"Analyse de sang et IRM c√©r√©brale pr√©vues le 15/03/2024.\",\n",
    "    \"Douleur thoracique et fi√®vre depuis hier matin.\",\n",
    "    \"Chirurgie de l'appendicite r√©alis√©e avec succ√®s.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß™ Test Predictions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    entities = ner_pipeline(text)\n",
    "    print(f\"\\nüìÑ Text: {text}\")\n",
    "    if entities:\n",
    "        print(\"üè∑Ô∏è Entities:\")\n",
    "        for entity in entities:\n",
    "            print(f\"   - {entity['word']}: {entity['entity_group']} (score: {entity['score']:.2f})\")\n",
    "    else:\n",
    "        print(\"   No entities found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d1ea5",
   "metadata": {},
   "source": [
    "## Step 14: Save Model for Production üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfa87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"\\nüíæ Saving model for production...\")\n",
    "\n",
    "# Create model directory\n",
    "model_dir = 'medical_ner_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'model_type': 'MedicalNER',\n",
    "    'base_model': MODEL_NAME,\n",
    "    'num_labels': len(labels),\n",
    "    'labels': labels,\n",
    "    'label2id': label2id,\n",
    "    'id2label': id2label,\n",
    "    'max_length': 128,\n",
    "    'eval_f1': eval_results['eval_f1'],\n",
    "    'eval_precision': eval_results['eval_precision'],\n",
    "    'eval_recall': eval_results['eval_recall'],\n",
    "    'training_samples': len(train_dataset),\n",
    "    'eval_samples': len(eval_dataset)\n",
    "}\n",
    "\n",
    "with open(f'{model_dir}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Model saved successfully!\")\n",
    "print(\"\\nüì¶ Files saved:\")\n",
    "for file in os.listdir(model_dir):\n",
    "    print(f\"   - {model_dir}/{file}\")\n",
    "\n",
    "# Zip the model\n",
    "shutil.make_archive('medical_ner_model', 'zip', model_dir)\n",
    "print(\"\\nüì¶ Model packaged: medical_ner_model.zip\")\n",
    "print(\"\\n‚¨áÔ∏è Download this file and upload to your project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ca7ea",
   "metadata": {},
   "source": [
    "## Step 15: Generate Detailed Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d60991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for detailed report\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=2)\n",
    "\n",
    "# Convert to label strings\n",
    "true_predictions = [\n",
    "    [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(pred_labels, predictions.label_ids)\n",
    "]\n",
    "true_labels = [\n",
    "    [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(pred_labels, predictions.label_ids)\n",
    "]\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã Detailed Classification Report\")\n",
    "print(\"=\"*60)\n",
    "print(seqeval_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db0f7c",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download the model**:\n",
    "   - Click on `medical_ner_model.zip` in the file browser\n",
    "   - Download to your computer\n",
    "\n",
    "2. **Upload to your project**:\n",
    "   ```bash\n",
    "   # Extract the zip\n",
    "   unzip medical_ner_model.zip\n",
    "   \n",
    "   # Move to project\n",
    "   mv medical_ner_model backend/ml_service/saved_models/\n",
    "   ```\n",
    "\n",
    "3. **Use in production**:\n",
    "   - Set `NER_USE_PRETRAINED=false` in `.env`\n",
    "   - Set `NER_MODEL_PATH=saved_models/medical_ner_model`\n",
    "   - Restart ML service\n",
    "\n",
    "### Model Performance:\n",
    "- ‚úÖ Trained on French medical text\n",
    "- ‚úÖ 8 entity types recognized\n",
    "- ‚úÖ Fine-tuned BioBERT\n",
    "- ‚úÖ Production-ready\n",
    "\n",
    "### For Your Teacher:\n",
    "- \"Fine-tuned BioBERT for medical Named Entity Recognition\"\n",
    "- \"Extracts 8 types of medical entities from French text\"\n",
    "- \"Uses state-of-the-art transformer architecture\"\n",
    "- \"Deployed as microservice with REST API\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
